{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87df0a3-2451-46b5-8c4f-d05576feaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.validation import make_valid\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aac0a-a7dd-4cd9-94bf-3a114f1dbbea",
   "metadata": {},
   "source": [
    "# Define functions\n",
    "\n",
    "Defines two functions for analyzing geographic overlaps:\n",
    "\n",
    "1. `compute_polygon_intersections()`: Computes new GeoDataFrame based on where boundaries overlap of two given GeoDataFrames\n",
    "2. `process_multiple_overlays()`: Computes overlap between a list of GeoDataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da35bc7b-b972-40ad-9bcb-64e8697421f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.validation import make_valid\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "import warnings\n",
    "\n",
    "def clean_geometry(geom):\n",
    "    \"\"\"\n",
    "    Clean and validate a geometry, handling various edge cases.\n",
    "    \n",
    "    Parameters:\n",
    "    geom: Shapely geometry object\n",
    "    \n",
    "    Returns:\n",
    "    Shapely geometry object: Cleaned and validated geometry\n",
    "    \"\"\"\n",
    "    if geom is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        # First try to make the geometry valid\n",
    "        valid_geom = make_valid(geom)\n",
    "        \n",
    "        # If the geometry is empty after validation, return None\n",
    "        if valid_geom.is_empty:\n",
    "            return None\n",
    "            \n",
    "        # Buffer by 0 to fix self-intersections and other topology issues\n",
    "        buffered = valid_geom.buffer(0)\n",
    "        \n",
    "        # Handle different geometry types\n",
    "        if isinstance(buffered, (Polygon, MultiPolygon)):\n",
    "            return buffered\n",
    "        elif hasattr(buffered, 'geoms'):\n",
    "            # Extract only polygon/multipolygon geometries\n",
    "            polygons = [g for g in buffered.geoms \n",
    "                       if isinstance(g, (Polygon, MultiPolygon))]\n",
    "            if polygons:\n",
    "                return unary_union(polygons)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Geometry cleaning failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def compute_polygon_intersections(gdf1, gdf2, target_crs='EPSG:5070'):\n",
    "    \"\"\"\n",
    "    Compute all new polygons formed by overlaying two geodataframes with enhanced error handling.\n",
    "    Uses Albers Equal Area projection (EPSG:5070) for accurate area calculations in the US.\n",
    "    \n",
    "    Parameters:\n",
    "    gdf1 (GeoDataFrame): First geodataframe\n",
    "    gdf2 (GeoDataFrame): Second geodataframe\n",
    "    target_crs (str): Target CRS for accurate area calculations (default: EPSG:5070 - Albers Equal Area)\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: A geodataframe containing all intersection polygons\n",
    "    \"\"\"\n",
    "    # Make copies to avoid modifying the original dataframes\n",
    "    gdf1 = gdf1.copy()\n",
    "    gdf2 = gdf2.copy()\n",
    "    \n",
    "    # Input validation\n",
    "    if not isinstance(gdf1, gpd.GeoDataFrame) or not isinstance(gdf2, gpd.GeoDataFrame):\n",
    "        raise ValueError(\"Both inputs must be GeoDataFrames\")\n",
    "    \n",
    "    if len(gdf1) == 0 or len(gdf2) == 0:\n",
    "        warnings.warn(\"One or both GeoDataFrames are empty\")\n",
    "        return None\n",
    "    \n",
    "    # Clean and validate geometries\n",
    "    gdf1['geometry'] = gdf1['geometry'].apply(clean_geometry)\n",
    "    gdf2['geometry'] = gdf2['geometry'].apply(clean_geometry)\n",
    "    \n",
    "    # Remove any rows where geometry cleaning failed\n",
    "    gdf1 = gdf1.dropna(subset=['geometry'])\n",
    "    gdf2 = gdf2.dropna(subset=['geometry'])\n",
    "    \n",
    "    if len(gdf1) == 0 or len(gdf2) == 0:\n",
    "        warnings.warn(\"No valid geometries remain after cleaning\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure CRS is set\n",
    "    if gdf1.crs is None or gdf2.crs is None:\n",
    "        warnings.warn(\"CRS not set for one or both GeoDataFrames\")\n",
    "        return None\n",
    "    \n",
    "    # Project to target CRS for accurate area calculations\n",
    "    try:\n",
    "        if gdf1.crs != target_crs:\n",
    "            gdf1 = gdf1.to_crs(target_crs)\n",
    "        if gdf2.crs != target_crs:\n",
    "            gdf2 = gdf2.to_crs(target_crs)\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"CRS transformation failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Perform the overlay operation with error handling\n",
    "    try:\n",
    "        intersection_gdf = gpd.overlay(\n",
    "            gdf1, \n",
    "            gdf2, \n",
    "            how='intersection',\n",
    "        )\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Overlay operation failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    if len(intersection_gdf) == 0:\n",
    "        warnings.warn(\"No intersections found between the geometries\")\n",
    "        return None\n",
    "    \n",
    "    # Filter for polygon/multipolygon geometries\n",
    "    polygon_types = ['Polygon', 'MultiPolygon']\n",
    "    intersection_gdf = intersection_gdf[\n",
    "        intersection_gdf.geometry.geom_type.isin(polygon_types)\n",
    "    ]\n",
    "    \n",
    "    # Final geometry validation\n",
    "    intersection_gdf['geometry'] = intersection_gdf['geometry'].apply(clean_geometry)\n",
    "    intersection_gdf = intersection_gdf.dropna(subset=['geometry'])\n",
    "    \n",
    "    if len(intersection_gdf) == 0:\n",
    "        warnings.warn(\"No valid polygons remain after final cleaning\")\n",
    "        return None\n",
    "    \n",
    "    # Add area calculations\n",
    "    intersection_gdf['area_sq_meters'] = intersection_gdf.geometry.area\n",
    "    \n",
    "    # Add a unique identifier for each new polygon\n",
    "    intersection_gdf['intersection_id'] = range(1, len(intersection_gdf) + 1)\n",
    "    \n",
    "    # Calculate the percentage of overlap with original polygons\n",
    "    for i, original_gdf in enumerate([gdf1, gdf2], 1):\n",
    "        overlap_areas = []\n",
    "        for geom in intersection_gdf.geometry:\n",
    "            try:\n",
    "                # Find all intersecting polygons from original\n",
    "                intersecting = original_gdf[original_gdf.intersects(geom)]\n",
    "                if len(intersecting) > 0:\n",
    "                    original_area = intersecting.iloc[0].geometry.area\n",
    "                    overlap_areas.append(\n",
    "                        (geom.area / original_area * 100) if original_area > 0 else 0\n",
    "                    )\n",
    "                else:\n",
    "                    overlap_areas.append(0)\n",
    "            except Exception:\n",
    "                overlap_areas.append(0)\n",
    "        \n",
    "        intersection_gdf[f'overlap_percent_{i}'] = overlap_areas\n",
    "    \n",
    "    return intersection_gdf\n",
    "\n",
    "def process_multiple_overlays(*gdfs, target_crs='EPSG:5070'):\n",
    "    \"\"\"\n",
    "    Process multiple overlays in sequence with enhanced error handling.\n",
    "    \n",
    "    Parameters:\n",
    "    *gdfs: Variable number of GeoDataFrames\n",
    "    target_crs (str): Target CRS for accurate area calculations\n",
    "    \n",
    "    Returns:\n",
    "    list: List of processed GeoDataFrames, where each entry is the result of\n",
    "          overlaying the previous result with the next input GeoDataFrame\n",
    "    \"\"\"\n",
    "    if len(gdfs) <= 1:\n",
    "        return list(gdfs)\n",
    "    \n",
    "    merged_gdfs = [gdfs[0]]\n",
    "    for i, gdf in enumerate(gdfs[1:], 1):\n",
    "        result = compute_polygon_intersections(merged_gdfs[-1], gdf, target_crs)\n",
    "        if result is None:\n",
    "            warnings.warn(f\"Overlay operation failed at step {i}\")\n",
    "            return merged_gdfs\n",
    "        merged_gdfs.append(result)\n",
    "    \n",
    "    return merged_gdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5740707-ed8c-47a4-9e6a-076288f9c92f",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Load geographic boundary files (source: https://www2.census.gov/geo/tiger/TIGER2024/) and process overlaps between:\n",
    "- Counties\n",
    "- ZIP codes\n",
    "- Congressional districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3500c3bb-bac7-4158-9ccf-30c3b6364df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/t9yd781506l5nzb9j3y9rrtm0000gn/T/ipykernel_23436/2540302427.py:99: UserWarning: `keep_geom_type=True` in overlay resulted in 51322 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  intersection_gdf = gpd.overlay(\n",
      "/var/folders/jq/t9yd781506l5nzb9j3y9rrtm0000gn/T/ipykernel_23436/2540302427.py:99: UserWarning: `keep_geom_type=True` in overlay resulted in 12313 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  intersection_gdf = gpd.overlay(\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "county_gdf = gpd.read_file('data/raw/tl_2024_us_county/tl_2024_us_county.shp')\n",
    "zip_gdf = gpd.read_file('data/raw/tl_2020_us_zcta520/tl_2020_us_zcta520.shp')\n",
    "district_gdf = gpd.read_file('data/raw/cb_2022_us_cd118_20m/cb_2022_us_cd118_20m.shp')\n",
    "\n",
    "# Process the overlays\n",
    "county_results, county_zip_results, county_zip_district_results = process_multiple_overlays(county_gdf, zip_gdf, district_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "0dccb29c-cedf-474c-baa8-6c602da8af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename the columns\n",
    "simplified_gdf = county_zip_district_results[['intersection_id', 'ZCTA5CE20', 'NAME', 'CD118FP', 'geometry',\n",
    "                                              'STATEFP_1', 'STATEFP_2']].copy()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "simplified_gdf = simplified_gdf.rename(columns={\n",
    "    'ZCTA5CE20': 'zip',\n",
    "    'NAME': 'county',\n",
    "    'CD118FP': 'district',\n",
    "    'STATEFP_1': 'county_state',\n",
    "    'STATEFP_2': 'district_state'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eacf9e-376a-485d-8f37-5b0aa0a2395e",
   "metadata": {},
   "source": [
    "# Resolve existing coordinates to intersected areas\n",
    "\n",
    "Functions to find which geographic area contains a given latitude/longitude point:\n",
    "- `find_containing_area()`: Core geospatial lookup function\n",
    "- `find_containing_area_details()`: User-friendly wrapper for getting area detailsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "22a15ea5-daa3-4ba9-9f69-26d5cd18c741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "\n",
    "def find_containing_area(lat, lon, gdf, source_crs='EPSG:4326'):\n",
    "    \"\"\"\n",
    "    Find if a lat-long point is contained within any polygon in the GeoDataFrame\n",
    "    and return the matching record(s).\n",
    "    \n",
    "    Parameters:\n",
    "    lat (float): Latitude of the point\n",
    "    lon (float): Longitude of the point\n",
    "    gdf (GeoDataFrame): GeoDataFrame containing polygons to check\n",
    "    source_crs (str): CRS of the input coordinates (default: EPSG:4326 for standard lat-long)\n",
    "    \n",
    "    Returns:\n",
    "    GeoDataFrame: Matching records where the point falls within the polygon\n",
    "                 Returns None if no matches or if there's an error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not (-90 <= lat <= 90) or not (-180 <= lon <= 180):\n",
    "            raise ValueError(\"Invalid latitude or longitude values\")\n",
    "        \n",
    "        if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "            raise TypeError(\"Input must be a GeoDataFrame\")\n",
    "            \n",
    "        # Create a Point from the coordinates\n",
    "        point = Point(lon, lat)  # Note: Point takes (x,y) which is (lon,lat)\n",
    "        \n",
    "        # Create a GeoDataFrame from the point\n",
    "        point_gdf = gpd.GeoDataFrame(\n",
    "            geometry=[point],\n",
    "            crs=source_crs\n",
    "        )\n",
    "        \n",
    "        # If the GeoDataFrames have different CRS, transform the point to match the polygons\n",
    "        if point_gdf.crs != gdf.crs:\n",
    "            point_gdf = point_gdf.to_crs(gdf.crs)\n",
    "        \n",
    "        # Find all polygons that contain the point\n",
    "        matches = gdf[gdf.geometry.contains(point_gdf.geometry[0])]\n",
    "        \n",
    "        if len(matches) == 0:\n",
    "            return None\n",
    "            \n",
    "        return matches\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing point ({lat}, {lon}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def find_containing_area_details(lat, lon, gdf):\n",
    "    \"\"\"\n",
    "    Wrapper function that returns a more user-friendly output.\n",
    "    \n",
    "    Parameters:\n",
    "    lat (float): Latitude of the point\n",
    "    lon (float): Longitude of the point\n",
    "    gdf (GeoDataFrame): GeoDataFrame containing polygons to check\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the area details, or None if no match\n",
    "    \"\"\"\n",
    "    matches = find_containing_area(lat, lon, gdf)\n",
    "    \n",
    "    if matches is None or len(matches) == 0:\n",
    "        return None\n",
    "        \n",
    "    # Create list of matching areas\n",
    "    results = []\n",
    "    for _, row in matches.iterrows():\n",
    "        result = {\n",
    "            'intersection_id': row['intersection_id'],\n",
    "            'zip': str(row['zip']),\n",
    "            'county': row['county'],\n",
    "            'district': str(row['district']),\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f47ceebb-e6ab-49d3-a732-6625471b97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full matching records:\n",
      "    intersection_id    zip  county district  \\\n",
      "13               14  68788  Cuming       01   \n",
      "\n",
      "                                             geometry county_state  \\\n",
      "13  POLYGON ((-49258.372 2082214.53, -49330.616 20...           31   \n",
      "\n",
      "   district_state  \n",
      "13             31  \n",
      "\n",
      "Simplified results:\n",
      "ZIP: 68788\n",
      "County: Cuming\n",
      "Congressional District: 01\n"
     ]
    }
   ],
   "source": [
    "lat = 41.9158651\n",
    "lon = -96.7885168\n",
    "\n",
    "matches = find_containing_area(lat, lon, simplified_gdf)\n",
    "if matches is not None:\n",
    "    print(\"\\nFull matching records:\")\n",
    "    print(matches)\n",
    "\n",
    "# Simplified version with just the key details\n",
    "results = find_containing_area_details(lat, lon, simplified_gdf)\n",
    "if results is not None:\n",
    "    print(\"\\nSimplified results:\")\n",
    "    for result in results:\n",
    "        print(f\"ZIP: {result['zip']}\")\n",
    "        print(f\"County: {result['county']}\")\n",
    "        print(f\"Congressional District: {result['district']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1e5fea-e5d6-4533-853e-e5d695091f96",
   "metadata": {},
   "source": [
    "# Build lookup table\n",
    "\n",
    "Associate coordinates previously used for Ballotpedia lookups by finding which census areas contain each ZIP code's coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "bc2ab2f0-747c-4585-ae7a-7291362b7a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 33704/33704 [00:57<00:00, 586.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "intersection_latlng_lookup = {}\n",
    "zip_latlng_lookup = {}\n",
    "def fill_latlng_lookup(row):\n",
    "    matches = find_containing_area_details(row.lat, row.lng, simplified_gdf)\n",
    "    for match in matches or []:\n",
    "        intersection_latlng_lookup[match['intersection_id']] = (row.lat, row.lng)\n",
    "        zip_latlng_lookup[row.zip] = (row.lat, row.lng)\n",
    "    return matches\n",
    "\n",
    "df = pd.read_csv('data/raw/zip_code_demographics.csv')\n",
    "df['matches'] = df.progress_apply(fill_latlng_lookup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "54e87b6e-2c3d-4651-af28-61af461b7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_lookup = dict(zip(df.zip.astype(int), df.population))\n",
    "state_lookup = {**dict(zip(df.zip.astype(int), df.state_name)), 32072: 'Florida', 97258: 'Oregon', 32026: 'Florida'}\n",
    "state_id_lookup = {**dict(zip(df.zip.astype(int), df.state_id)), 32072: 'FL', 97258: 'OR', 32026: 'FL'}\n",
    "\n",
    "simplified_gdf['population'] = simplified_gdf.zip.astype(int).map(population_lookup)\n",
    "simplified_gdf['state'] = simplified_gdf.zip.astype(int).map(state_lookup)\n",
    "simplified_gdf['state_id'] = simplified_gdf.zip.astype(int).map(state_id_lookup)\n",
    "\n",
    "# Remove all NAs with zip codes starting with 00 (these are in Puerto Rico, which has no ballots)\n",
    "simplified_gdf = simplified_gdf[~(simplified_gdf.zip.str.startswith('00') & simplified_gdf.state.isna())]\n",
    "\n",
    "assert simplified_gdf.state.notna().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e06b02-55a2-44c9-9cee-804abe60e1f5",
   "metadata": {},
   "source": [
    "# Fix states for messed up intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "4d359fa3-1a3f-4af9-8f02-5aabf777be32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "county_state_lookup = dict(simplified_gdf.groupby('county_state').state_id.apply(lambda x: x.mode()[0]))\n",
    "district_state_lookup = dict(simplified_gdf.groupby('district_state').state_id.apply(lambda x: x.mode()[0]))\n",
    "len(county_state_lookup)\n",
    "len(district_state_lookup)\n",
    "county_state_lookup == district_state_lookup\n",
    "\n",
    "simplified_gdf['county_state_id'] = simplified_gdf.county_state.map(county_state_lookup)\n",
    "simplified_gdf['district_state_id'] = simplified_gdf.district_state.map(district_state_lookup)\n",
    "simplified_gdf = simplified_gdf[\n",
    "    (simplified_gdf.district_state_id == simplified_gdf.state_id) &\n",
    "    (simplified_gdf.county_state_id == simplified_gdf.state_id)\n",
    "].drop(columns=['county_state', 'district_state', 'county_state_id', 'district_state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "3e667163-f260-4dc3-9010-3a5a52f65c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "      <th>population</th>\n",
       "      <th>density</th>\n",
       "      <th>county_name</th>\n",
       "      <th>po_box</th>\n",
       "      <th>dist_highway</th>\n",
       "      <th>dist2_large_airport</th>\n",
       "      <th>dist2_medium_airport</th>\n",
       "      <th>dist_to_shore</th>\n",
       "      <th>number_of_business</th>\n",
       "      <th>adjusted_gross_income</th>\n",
       "      <th>total_income_amount</th>\n",
       "      <th>number_of_returns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>42.06262</td>\n",
       "      <td>-72.62521</td>\n",
       "      <td>Agawam</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>16088</td>\n",
       "      <td>550.1</td>\n",
       "      <td>Hampden</td>\n",
       "      <td>0</td>\n",
       "      <td>1.387035</td>\n",
       "      <td>106.145765</td>\n",
       "      <td>12.946212</td>\n",
       "      <td>93.049251</td>\n",
       "      <td>438.0</td>\n",
       "      <td>598807</td>\n",
       "      <td>6.047690e+05</td>\n",
       "      <td>9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>42.37633</td>\n",
       "      <td>-72.46462</td>\n",
       "      <td>Amherst</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>27323</td>\n",
       "      <td>198.1</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>0</td>\n",
       "      <td>14.438177</td>\n",
       "      <td>112.264368</td>\n",
       "      <td>21.080079</td>\n",
       "      <td>133.370144</td>\n",
       "      <td>571.0</td>\n",
       "      <td>989558</td>\n",
       "      <td>1.005796e+06</td>\n",
       "      <td>9880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005</td>\n",
       "      <td>42.42117</td>\n",
       "      <td>-72.10655</td>\n",
       "      <td>Barre</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>4947</td>\n",
       "      <td>44.2</td>\n",
       "      <td>Worcester</td>\n",
       "      <td>0</td>\n",
       "      <td>16.788339</td>\n",
       "      <td>90.664964</td>\n",
       "      <td>25.547718</td>\n",
       "      <td>97.639881</td>\n",
       "      <td>97.0</td>\n",
       "      <td>164207</td>\n",
       "      <td>1.660540e+05</td>\n",
       "      <td>2490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007</td>\n",
       "      <td>42.28163</td>\n",
       "      <td>-72.40009</td>\n",
       "      <td>Belchertown</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>15304</td>\n",
       "      <td>107.7</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>0</td>\n",
       "      <td>13.663839</td>\n",
       "      <td>101.552921</td>\n",
       "      <td>14.762395</td>\n",
       "      <td>114.406034</td>\n",
       "      <td>217.0</td>\n",
       "      <td>647074</td>\n",
       "      <td>6.547390e+05</td>\n",
       "      <td>7970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1008</td>\n",
       "      <td>42.18234</td>\n",
       "      <td>-72.95819</td>\n",
       "      <td>Blandford</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>1171</td>\n",
       "      <td>7.4</td>\n",
       "      <td>Hampden</td>\n",
       "      <td>0</td>\n",
       "      <td>2.593655</td>\n",
       "      <td>136.548797</td>\n",
       "      <td>20.177950</td>\n",
       "      <td>107.466779</td>\n",
       "      <td>18.0</td>\n",
       "      <td>47826</td>\n",
       "      <td>4.824100e+04</td>\n",
       "      <td>660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    zip       lat       lng         city state_id     state_name  population  \\\n",
       "0  1001  42.06262 -72.62521       Agawam       MA  Massachusetts       16088   \n",
       "1  1002  42.37633 -72.46462      Amherst       MA  Massachusetts       27323   \n",
       "2  1005  42.42117 -72.10655        Barre       MA  Massachusetts        4947   \n",
       "3  1007  42.28163 -72.40009  Belchertown       MA  Massachusetts       15304   \n",
       "4  1008  42.18234 -72.95819    Blandford       MA  Massachusetts        1171   \n",
       "\n",
       "   density county_name  po_box  dist_highway  dist2_large_airport  \\\n",
       "0    550.1     Hampden       0      1.387035           106.145765   \n",
       "1    198.1   Hampshire       0     14.438177           112.264368   \n",
       "2     44.2   Worcester       0     16.788339            90.664964   \n",
       "3    107.7   Hampshire       0     13.663839           101.552921   \n",
       "4      7.4     Hampden       0      2.593655           136.548797   \n",
       "\n",
       "   dist2_medium_airport  dist_to_shore  number_of_business  \\\n",
       "0             12.946212      93.049251               438.0   \n",
       "1             21.080079     133.370144               571.0   \n",
       "2             25.547718      97.639881                97.0   \n",
       "3             14.762395     114.406034               217.0   \n",
       "4             20.177950     107.466779                18.0   \n",
       "\n",
       "   adjusted_gross_income  total_income_amount  number_of_returns  \n",
       "0                 598807         6.047690e+05               9320  \n",
       "1                 989558         1.005796e+06               9880  \n",
       "2                 164207         1.660540e+05               2490  \n",
       "3                 647074         6.547390e+05               7970  \n",
       "4                  47826         4.824100e+04                660  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_zip_df = demographics_df[(~demographics_df.zip.astype(str).isin(simplified_gdf.zip) & \n",
    "                                  ~demographics_df.zip.astype(str).str.startswith('00'))]\n",
    "missing_zip_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d8bdfede-cd9d-4fce-99b3-e9ee1a335198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get centroids and convert to lat-lng if needed\n",
    "if simplified_gdf.crs != 'EPSG:4326':\n",
    "    # Convert to lat-lng projection first\n",
    "    centroids = simplified_gdf.geometry.centroid.to_crs('EPSG:4326')\n",
    "else:\n",
    "    centroids = simplified_gdf.geometry.centroid\n",
    "\n",
    "# Add the centerpoints of each intersection\n",
    "simplified_gdf['center_latlng'] = [(c.y, c.x) for c in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b1f2de2e-31f5-4a2a-ab4e-d02cd40e403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new centerpoints as fallbacks\n",
    "simplified_gdf['lookup_latlng'] = simplified_gdf.intersection_id.map(match_lookup).fillna(simplified_gdf.center_latlng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "4971f0f8-d95e-42a5-a8c8-515d9e467a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/t9yd781506l5nzb9j3y9rrtm0000gn/T/ipykernel_23436/2834953364.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_zip_df['intersection_id'] = list(range(max_id + 1, max_id + len(missing_zip_df) + 1))\n"
     ]
    }
   ],
   "source": [
    "max_id = county_zip_district_results.intersection_id.max().astype(int)\n",
    "missing_zip_df['intersection_id'] = list(range(max_id + 1, max_id + len(missing_zip_df) + 1))\n",
    "simplified_gdf = pd.concat([\n",
    "    simplified_gdf,\n",
    "    pd.DataFrame(missing_zip_df.apply(\n",
    "        lambda row: {'intersection_id': row.intersection_id, 'zip': row.zip, 'county': row.county_name,\n",
    "                     'population': row.population, 'state': row.state_name, 'state_id': row.state_id,\n",
    "                     'center_latlng': (row.lat, row.lng), 'lookup_latlng': (row.lat, row.lng)},\n",
    "        axis=1\n",
    "    ).to_list())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31814b7-fcb9-4c75-a737-a99875c8ee68",
   "metadata": {},
   "source": [
    "# Get street maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a0c6bcb7-3ae6-4f38-8784-3c6e29dbbfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "def generate_distinct_colors(n: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate n visually distinct colors using a professional color palette\n",
    "    \"\"\"\n",
    "    colors = [\n",
    "        '#4e79a7',  # Blue\n",
    "        '#f28e2b',  # Orange\n",
    "        '#59a14f',  # Green\n",
    "        '#e15759',  # Red\n",
    "        '#76b7b2',  # Teal\n",
    "        '#6c5b7b',  # Purple\n",
    "        '#b07aa1',  # Pink\n",
    "        '#9c755f',  # Brown\n",
    "        '#86bcb6',  # Light teal\n",
    "        '#4b4b8f',  # Navy\n",
    "    ]\n",
    "    while len(colors) < n:\n",
    "        colors.append(colors[len(colors) % len(colors)])\n",
    "    return colors[:n]\n",
    "\n",
    "def create_map_with_polygons(polygons_data: List[Dict], zoom_start: int = None, filename: str = 'map_with_polygons'):\n",
    "    \"\"\"\n",
    "    Create an interactive map with multiple labeled polygons\n",
    "    \n",
    "    Args:\n",
    "        polygons_data: List of dictionaries, each containing:\n",
    "            - 'coordinates': List of [longitude, latitude] coordinates\n",
    "            - 'name': Name of the polygon\n",
    "            - 'additional_info': (optional) Additional information for popup\n",
    "        zoom_start: Initial zoom level (optional, will auto-fit if None)\n",
    "    \n",
    "    Returns:\n",
    "        Path to saved HTML file\n",
    "    \"\"\"\n",
    "    # Calculate center point from all polygons\n",
    "    all_lats = [coord[1] for poly in polygons_data \n",
    "                for coord in poly['coordinates']]\n",
    "    all_lons = [coord[0] for poly in polygons_data \n",
    "                for coord in poly['coordinates']]\n",
    "\n",
    "    if not all_lats or not all_lons:\n",
    "        return None\n",
    "\n",
    "    center_lat = np.mean(all_lats)\n",
    "    center_lon = np.mean(all_lons)\n",
    "    \n",
    "    # Create base map\n",
    "    m = folium.Map(\n",
    "        location=[center_lat, center_lon],\n",
    "        zoom_start=zoom_start if zoom_start is not None else 13,\n",
    "        tiles='CartoDB positron',\n",
    "        control_scale=True\n",
    "    )\n",
    "    \n",
    "    # Get unique names and assign colors\n",
    "    unique_names = list(set(poly['name'] for poly in polygons_data))\n",
    "    colors = generate_distinct_colors(len(unique_names))\n",
    "    name_to_color = dict(zip(unique_names, colors))\n",
    "    \n",
    "    # Calculate bounds for all polygons\n",
    "    sw_bound = [min(all_lats), min(all_lons)]\n",
    "    ne_bound = [max(all_lats), max(all_lons)]\n",
    "    \n",
    "    # Add polygons to map\n",
    "    for poly_data in polygons_data:\n",
    "        folium_coords = [[lat, lon] for lon, lat in poly_data['coordinates']]\n",
    "        color = name_to_color[poly_data['name']]\n",
    "        \n",
    "        popup_content = f\"\"\"\n",
    "            <div style=\"font-family: Arial, sans-serif;\">\n",
    "                <h4 style=\"margin:0 0 5px 0;\">{poly_data['name']}</h4>\n",
    "                {f'<p style=\"margin:0;\">{poly_data[\"additional_info\"]}</p>' if 'additional_info' in poly_data else ''}\n",
    "            </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.Polygon(\n",
    "            locations=folium_coords,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            fill_opacity=0.2,\n",
    "            weight=2,\n",
    "            popup=folium.Popup(popup_content, max_width=300),\n",
    "            tooltip=poly_data['name']\n",
    "        ).add_to(m)\n",
    "    \n",
    "    # Fit the map to the bounds of all polygons with some padding\n",
    "    m.fit_bounds([sw_bound, ne_bound], padding=(30, 30))\n",
    "    \n",
    "    # Create legend with unique entries\n",
    "    legend_html = \"\"\"\n",
    "        <div style=\"\n",
    "            position: fixed;\n",
    "            bottom: 50px;\n",
    "            right: 50px;\n",
    "            z-index: 1000;\n",
    "            background-color: white;\n",
    "            padding: 10px;\n",
    "            border: 2px solid rgba(0,0,0,0.2);\n",
    "            border-radius: 4px;\n",
    "            font-family: Arial, sans-serif;\n",
    "            font-size: 12px;\n",
    "            max-width: 200px;\n",
    "            box-shadow: 0 1px 5px rgba(0,0,0,0.2);\n",
    "            \">\n",
    "            <div style=\"font-weight: bold; margin-bottom: 10px;\">Areas</div>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add legend items for unique names only\n",
    "    for name, color in name_to_color.items():\n",
    "        legend_html += f\"\"\"\n",
    "            <div style=\"\n",
    "                display: flex;\n",
    "                align-items: center;\n",
    "                margin-bottom: 5px;\n",
    "                white-space: nowrap;\n",
    "                overflow: hidden;\n",
    "                text-overflow: ellipsis;\n",
    "            \">\n",
    "                <span style=\"\n",
    "                    display: inline-block;\n",
    "                    width: 12px;\n",
    "                    height: 12px;\n",
    "                    margin-right: 8px;\n",
    "                    background: {color};\n",
    "                    opacity: 0.7;\n",
    "                    border-radius: 2px;\n",
    "                \"></span>\n",
    "                <span>{name}</span>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    legend_html += \"</div>\"\n",
    "    m.get_root().html.add_child(folium.Element(legend_html))\n",
    "    \n",
    "    # Save the map\n",
    "    output_file = f'{filename}.html'\n",
    "    m.save(f'data/processed/{output_file}')\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "0c7c53fc-788b-4fa3-9f64-cf307b403bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from typing import List, Tuple\n",
    "\n",
    "def albers_to_latlon(coords: List[Tuple[float, float]]) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Convert coordinates from NAD83 / Conus Albers (EPSG:5070) to WGS84 latitude/longitude\n",
    "    \n",
    "    Args:\n",
    "        coords: List of (x, y) tuples in Albers projection (meters)\n",
    "        \n",
    "    Returns:\n",
    "        List of [longitude, latitude] coordinates\n",
    "    \"\"\"\n",
    "    # Create transformer from NAD83 / Conus Albers to WGS84\n",
    "    transformer = pyproj.Transformer.from_crs(\n",
    "        \"EPSG:5070\",  # NAD83 / Conus Albers\n",
    "        \"EPSG:4326\",  # WGS84\n",
    "        always_xy=True\n",
    "    )\n",
    "    \n",
    "    # Transform coordinates\n",
    "    latlon_coords = []\n",
    "    for x, y in coords:\n",
    "        lon, lat = transformer.transform(x, y)\n",
    "        if np.isnan(lat) or np.isnan(lon):\n",
    "            continue\n",
    "        latlon_coords.append([lon, lat])\n",
    "    \n",
    "    return latlon_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7a63ea6f-adbe-4af4-87f6-dd857f58131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 36151/36151 [26:07<00:00, 23.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import shapely\n",
    "from shapely.ops import transform\n",
    "import numpy as np\n",
    "\n",
    "def get_polygon_min_width(polygon):\n",
    "    \"\"\"\n",
    "    Estimates the minimum width of a polygon by sampling points and finding\n",
    "    minimum distance to boundary. This is an approximation but works well\n",
    "    for detecting narrow slivers.\n",
    "    \n",
    "    Args:\n",
    "        polygon: shapely Polygon object\n",
    "    Returns:\n",
    "        float: Approximate minimum width of the polygon\n",
    "    \"\"\"\n",
    "    if not polygon:\n",
    "        return None\n",
    "        \n",
    "    if not polygon.is_valid:\n",
    "        return 0\n",
    "    \n",
    "    # Get a grid of points inside the polygon\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    x_range = np.linspace(minx, maxx, 20)\n",
    "    y_range = np.linspace(miny, maxy, 20)\n",
    "    xx, yy = np.meshgrid(x_range, y_range)\n",
    "    points = [shapely.Point(x, y) for x, y in zip(xx.flat, yy.flat)]\n",
    "    \n",
    "    # Filter to points inside the polygon\n",
    "    points = [p for p in points if polygon.contains(p)]\n",
    "    \n",
    "    if not points:\n",
    "        return 0\n",
    "    \n",
    "    # Calculate distance to boundary for each point\n",
    "    distances = [polygon.boundary.distance(p) for p in points]\n",
    "    \n",
    "    # Return maximum distance (represents half the minimum width)\n",
    "    return max(distances) * 2\n",
    "\n",
    "# Modified version of your original code\n",
    "polygon_lookup = simplified_gdf.groupby('zip').progress_apply(\n",
    "    lambda df: df.apply(lambda row: [{\n",
    "        'name': f' {row.county} - {row.state_id}-CD{row.district}'.replace('00', '01'),\n",
    "        'coordinates': albers_to_latlon(x.exterior.coords)\n",
    "    } for x in (\n",
    "        row.geometry.geoms if type(row.geometry) == shapely.geometry.multipolygon.MultiPolygon\n",
    "        else [row.geometry]\n",
    "    ) if (get_polygon_min_width(x) or 0) > 1e-10  # Adjust threshold as needed (in same units as your CRS)\n",
    "    ], axis=1).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8d08fd31-547d-4e02-ae45-8f73378313f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 63374/63374 [17:10<00:00, 61.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Omit insignificant regions (e.g. streets)\n",
    "simplified_gdf['best_width'] = simplified_gdf.geometry.progress_apply(\n",
    "    lambda x: max([get_polygon_min_width(y) for y in (\n",
    "        [x] if type(x) != shapely.geometry.multipolygon.MultiPolygon\n",
    "        else x.geoms\n",
    "    )])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fafbe54c-a941-4485-8c9b-4b24616ec816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63374"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "59527"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simplified_gdf = \n",
    "# simplified_gdf.best_width.plot.hist(bins=1000)\n",
    "len(simplified_gdf)\n",
    "sample_gdf = simplified_gdf[(simplified_gdf.best_width > 10) | simplified_gdf.best_width.isna()]\n",
    "len(sample_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "7fc6eadb-dd51-4295-8fbc-9100a17a064f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.016370124340668"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300\" height=\"300\" viewBox=\"839586.0742424825 1994169.1773624816 10186.320185176679 6618.544240218354\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,3994956.898965182)\"><g><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"67.90880123451119\" opacity=\"0.6\" d=\"M 848943.3816484006,1995619.6020712506 L 848924.3721716134,1995617.3425932352 L 848922.9530208821,1995617.190733651 L 848920.2816782527,1995616.904881004 L 848898.2550154078,1995614.4352566889 L 848857.0518893292,1995609.6885119963 L 848736.924595827,1995595.2588573922 L 848604.0602974328,1995578.3421127845 L 848602.9750591874,1995578.2260286608 L 848450.456883227,1995559.5475626574 L 848314.2647035853,1995542.955732359 L 848209.9020497827,1995529.5440052077 L 848105.6826099966,1995517.951356187 L 847969.1557052583,1995501.3297530569 L 847813.7019636766,1995481.674051292 L 847809.6233025674,1995481.1255357813 L 847496.4723626013,1995441.4712701093 L 847426.6817554523,1995433.227660706 L 847336.8911980423,1995422.5106173104 L 847042.0079793483,1995384.1497867955 L 847022.0676918243,1995381.9081998356 L 846886.6118037502,1995366.3210239138 L 846714.6495510306,1995345.4881994007 L 846552.1084341226,1995325.7770741112 L 846511.1063255793,1995320.726195773 L 846417.2712431947,1995309.1375807887 L 846416.2694508223,1995309.030702338 L 846414.6116794226,1995308.7411938044 L 846353.525769712,1995301.210551007 L 846303.7221760933,1995295.5597672185 L 846300.7167937922,1995295.2391752587 L 846298.2123084464,1995294.9720161618 L 846223.1252284803,1995286.5120974898 L 845999.532454756,1995259.7375422507 L 845837.002328232,1995242.2953478512 L 845588.1126057528,1995212.8355954315 L 845584.7016444658,1995212.359406328 L 845582.7815189839,1995212.154757574 L 845581.6962306473,1995212.039086744 L 845574.4450175295,1995211.1536064537 L 845555.0170473284,1995208.8577135177 L 845430.1960589103,1995194.0912516818 L 845277.9445709056,1995176.8548115457 L 845183.8337852918,1995166.2654137765 L 845181.7466802478,1995166.0430749543 L 845180.4944171933,1995165.909671925 L 845178.5742804697,1995165.7051209968 L 845175.0798088859,1995165.2202144864 L 845173.9110299145,1995165.095705968 L 845064.3071632577,1995151.730864959 L 845025.5704131644,1995147.6049294844 L 844998.8554013629,1995144.7595669406 L 844995.5995086455,1995144.4127945302 L 844944.852819542,1995138.8954940867 L 844856.072253717,1995128.2018404894 L 844714.4219944939,1995112.1052661804 L 844580.4036420916,1995095.6972583001 L 844434.1605841129,1995078.3283393295 L 844179.0269666808,1995047.2350529633 L 844082.0879059564,1995036.2453215134 L 844061.2639089137,1995033.5793299137 L 844054.1913316789,1995032.6016209887 L 844035.955378123,1995030.2110282283 L 844032.3773449579,1995029.7177435374 L 844030.0397470746,1995029.469063352 L 843957.2391347094,1995020.1476906522 L 843836.9118350237,1995006.7857871412 L 843741.1523289081,1994995.0245979927 L 843680.7439711402,1994988.2627323926 L 843588.8014756675,1994977.9230766415 L 843584.9611111785,1994977.5147456955 L 843581.5381774454,1994977.1508001145 L 843522.298484439,1994970.5144148383 L 843446.7783029852,1994961.3592215576 L 843404.5217498312,1994956.191269221 L 843355.813028228,1994950.5629813578 L 843347.7266796521,1994949.5907929789 L 843336.8017892316,1994948.3168919762 L 843334.6311403573,1994948.0861649397 L 843330.7907614262,1994947.6779570223 L 843234.1961962759,1994936.6226989196 L 843231.6081111035,1994936.3476348375 L 843226.0263451365,1994935.6417615258 L 843224.8575323455,1994935.517540136 L 843223.5217462756,1994935.3755730442 L 843199.8352143683,1994932.6329288036 L 843197.4975878805,1994932.3844939126 L 843194.9095013267,1994932.1094418017 L 843130.5169506851,1994924.7031142807 L 842934.9289707965,1994901.3301036824 L 842926.9260530546,1994900.3672140767 L 842924.6719040606,1994900.127729386 L 842922.8470235289,1994899.8212108798 L 842921.7616924667,1994899.7059039134 L 842161.7543270848,1994808.8610657102 L 842156.3394321373,1994808.1736684677 L 842152.9164253447,1994807.8103392343 L 842151.246665878,1994807.6331059982 L 841736.9727958553,1994758.1525283705 L 841711.9616926447,1994755.161224386 L 841707.2146629099,1994754.5449899333 L 841700.046465056,1994753.671907701 L 841561.7278755071,1994737.1973452503 L 841548.7272631523,1994735.5931350335 L 841532.9715190228,1994733.69672578 L 840227.0323936023,1994577.7210524774 L 840205.0263123758,1994575.0527899545 L 839964.65321413,1994546.448480451 L 839963.3453604521,1994558.4259370035 L 849395.1233096897,1995674.7149201152 L 849205.8525920354,1995652.3120979783 L 848948.1280575609,1995620.2226349867 L 848943.3816484006,1995619.6020712506 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"67.90880123451119\" opacity=\"0.6\" d=\"M 849285.1919492268,1996613.9357851218 L 849285.3946611843,1996612.8309925161 L 849313.667232181,1996370.0563735587 L 849313.9546038231,1996367.3723301026 L 849275.0219933374,1996699.6058042627 L 849276.2248983389,1996689.8027412028 L 849281.245012926,1996647.6460334426 L 849285.1919492268,1996613.9357851218 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"67.90880123451119\" opacity=\"0.6\" d=\"M 849052.9248143375,1998605.9344708023 L 849072.8850312275,1998436.846092627 L 849136.987064202,1997909.0792021935 L 849151.1527597555,1997783.8659924965 L 849155.2546243822,1997747.1310480754 L 849159.3564461892,1997713.5502588793 L 849161.3000419672,1997697.7622117864 L 849167.5004948882,1997646.156981326 L 849167.8105221008,1997643.2613029203 L 849171.9958276914,1997608.900888006 L 849181.0700279047,1997529.6663619888 L 849181.296586702,1997527.5502878432 L 849183.0494173843,1997512.7556393393 L 849188.9279794925,1997461.7917809915 L 849189.2260835014,1997459.007471777 L 849195.1762064889,1997406.5868203547 L 849196.0705107026,1997399.0224324416 L 849196.3805306762,1997396.9152924255 L 849196.320935187,1997395.1065264791 L 849197.2629194265,1997388.6737338086 L 849198.0260581597,1997382.3344417398 L 849199.1826948609,1997372.3198593843 L 849209.4255063182,1997282.9592632733 L 849214.1235964325,1997243.8097590648 L 849215.0894428097,1997236.3656725176 L 849216.6276445996,1997224.3642482008 L 849224.6286834906,1997163.038516176 L 849224.9983283142,1997160.3745106952 L 849227.2281290801,1997143.4905603097 L 849227.4427654361,1997141.485853913 L 849227.7885622062,1997139.044593286 L 849230.6146081248,1997112.6492897496 L 849241.7399374758,1997007.9500784555 L 849248.8706198799,1996945.2919358779 L 849254.1172959907,1996896.2879361454 L 849254.3677055712,1996893.9491085229 L 849255.0831615353,1996887.266743701 L 849257.3845521654,1996864.194710133 L 849258.3742662407,1996854.95077037 L 849259.1016465002,1996848.1570311766 L 849266.1850316088,1996775.0164808005 L 849031.6427471214,1998776.495370617 L 849034.0499910068,1998756.2139950905 L 849035.0634766818,1998748.3244856463 L 849052.9248143375,1998605.9344708023 z\" /><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"67.90880123451119\" opacity=\"0.6\" d=\"M 848969.5184992538,1999352.6472647195 L 849007.32911918,1998990.8152340897 L 849014.8768426824,1998921.8942181102 L 849015.7711064059,1998914.3298964533 L 849018.7375666864,1998886.622404923 L 848840.1685491351,2000410.4504847305 L 848866.8837850903,2000199.3562777338 L 848921.8720887572,1999772.4526898575 L 848969.5184992538,1999352.6472647195 z\" /></g></g></svg>"
      ],
      "text/plain": [
       "<MULTIPOLYGON (((848943.382 1995619.602, 848924.372 1995617.343, 848922.953 ...>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'46919'"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_gdf.sort_values(by='best_width').iloc[0].best_width\n",
    "sample_gdf.sort_values(by='best_width').iloc[0].geometry\n",
    "sample_gdf.sort_values(by='best_width').iloc[0].zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c997f88b-1148-413f-a9be-a71ffe454865",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lookup = {}\n",
    "for zip_code, x in dict(polygon_lookup).items():\n",
    "    if len(set(y['name'] for y in x)) > 1:\n",
    "        output_file = create_map_with_polygons(x, filename=zip_code)\n",
    "        output_lookup[zip_code] = output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0b451c09-7b0c-43ae-9e88-ede7fd93552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for x in os.listdir('data/processed'):\n",
    "    if x.endswith('html'):\n",
    "        if x.split('.')[-2] not in output_lookup:\n",
    "            os.remove(f'data/processed/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8d6b1807-b0f3-4baa-82ff-6fa70b9ec71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericbolton/Documents/showballot/env/lib/python3.12/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "sample_gdf['link'] = sample_gdf.zip.map(output_lookup).apply(\n",
    "    lambda x: f'https://edbltn.github.io/show-me-the-ballot/data/processed/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "db7667d0-64fe-4c87-984c-dab4ea7b9993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/t9yd781506l5nzb9j3y9rrtm0000gn/T/ipykernel_23436/1250419676.py:1: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  sample_gdf.to_file('data/processed/areas.shp', index=False)\n",
      "/Users/ericbolton/Documents/showballot/env/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'intersection_id' to 'intersecti'\n",
      "  ogr_write(\n",
      "/Users/ericbolton/Documents/showballot/env/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'center_latlng' to 'center_lat'\n",
      "  ogr_write(\n",
      "/Users/ericbolton/Documents/showballot/env/lib/python3.12/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'lookup_latlng' to 'lookup_lat'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "sample_gdf.to_file('data/processed/areas.shp', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
